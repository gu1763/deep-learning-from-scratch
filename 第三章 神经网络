第三章 神经网络
#从感知机到神经网络
用图来表示神经网络，把最左边的一列称为输入层，最右边为输出层，中间的一列称为中间层（隐藏层）。
感知机中的偏置也可以如上图中表示。输入信号之和会被h（x）（当x<=0时为0，>0时为1）转换，转换后的值就是输出y。

#激活函数
作用如h（x）的函数被称为激活函数。
阶跃函数：以阈值为界，一旦输入超过阈值，就切换输出的函数。可以用数组的><运算来实现。图像呈阶梯式变化。
sigmoid函数：h（x）=1/（1+exp（-x））。

·神经网络的激活函数必须使用非线性函数。因为用线性函数的话，加深层数就没有意义了。（如把y=cx叠三层，可以用y=c^3x代替。）
RELU函数：输入值大于0时直接输出该值，小于零时输出0。（可用numpy中的maximum函数来实现。

#多维数组的运算
n维数组的维数可由np.ndim（）函数获得。
n维数组形状可用数组名.shape获取。

#三层神经网络的实现
#符号确认
右下标先后在前，可与矩阵匹配。
（下面看原书）
#输出层的实现
恒等函数：恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。
softmax函数：分类问题中使用的 softmax 函数可以用下面的式表示。（输出层中各神经元都受到所有输入信号的影响。
#实现 softmax 函数时的注意事项
当ai过大，会溢出，无法正确计算。（计 算 机 处 理“ 数 ”时 ，数 值 必 须 在 4 字 节 或 8 字 节 的 有 限 数 据 宽 度 内 。）
改进：分子分母同时乘小量c，变为exp（ai+ln c）。

#softmax函数的特征
softmax 函数的输出是 0.0 到 1.0 之间的实数。并且，softmax 函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质。正 因为有了这个性质，我们才可以把 softmax 函数的输出解释为“概率”。
一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。 并且，即便使用 softmax 函数，输出值最大的神经元的位置也不会变。因此， 神经网络在进行分类时，输出层的 softmax 函数可以省略。

#前向传播
假设学习已经全部结束，我们使用学习到的参 数，先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向 传播(forward propagation)。

#预处理
把数据限定到某 个范围内的处理称为正规化(normalization)。此外，对神经网络的输入数据 进行某种既定的转换称为预处理(pre-processing)。

#批处理
打包式的输入数据称为批(batch)。使用批处理，可以实现高速
且高效的运算。
