第四章 神经网络的学习
·学习：从训练数据中自动获取最优参数的过程。

#数据驱动
数据是机器学习的核心。
设计识别5的算法:一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量。
特征量：这里所说的“特征量”是指可以 从输入数据(输入图像)中准确地提取本质数据(重要的数据)的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括 SIFT、SURF 和 HOG 等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的 SVM、KNN 等分类器进行学习。
即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。
·深度学习有时也称为端到端机器学习(end-to-end machine  learning)。这里所说的端到端是指从一端到另一端的意思，也就是 从原始数据(输入)中获得目标结果(输出)的意思。

#训练数据和测试数据
机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和 实验等。首先，使用训练数据进行学习，寻找最优的参数;然后，使用测试 数据评价训练得到的模型的实际能力。为什么需要将数据分为训练数据和测 试数据呢?因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能 力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。
泛化能力：泛化能力是指处理未被观察过的数据(不包含在训练数据中的数据)的能力。
过拟合：仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。 这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。顺 便说一下，只对某个数据集过度拟合的状态称为过拟合(over fitting)。

#损失函数
神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数(loss function)。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。
均方误差：
这里，yk 是表示神经网络的输出，tk 表示监督数据，k 表示数据的维数。
one-hot表示：将正确解标签表示为 1，其他标签表示为 0 的表示方法称为 one-hot 表示。

#交叉熵误差
这里，log 表示以 e 为底数的自然对数(log e)。yk 是神经网络的输出，tk 是 正 确 解 标 签。并 且 ，t k 中只有正确解 标 签 的 索 引 为 1 ，其 他 均 为 0 ( o n e - h o t 表 示 )。 因此，式(4.2)实际上只计算对应正确解标签的输出的自然对数。
·函数内部在计算 np.log 时，有时会加上一个微小值 delta。这是因为，当出现 np.log(0) 时，np.log(0) 会变为负无限大 的 -inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。

#mini-batch学习
机器学习使用训练数据进行学习。使用训练数据进行学习，严格来说， 就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。因此， 计算损失函数时必须将所有的训练数据作为对象。
前面介绍的损失函数的例子中考虑的都是针对单个数据的损失函数。如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式：
这里 , 假设数据有 N 个，tnk 表示第 n 个数据的第 k 个元素的值(ynk 是神 经网络的输出，tnk 是监督数据)。

mini-batch学习：从训练数据中选出一批数据(称为mini-batch,小批量)，然后对每个 mini-batch 进行学习。
（p91，mini-batch版交叉熵误差的实现）

#为何要设定损失函数
识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值 也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出 于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。
通过求导使损失函数尽可能小，从而得到更优参数。

#使用numpy求导数
numrical_diff（function，x0）
偏导数：将其他未知量带入数值，再对目标未知量求导。

#梯度
定义：这样的由全部变量的偏导数汇总 而成的向量称为梯度(gradient)。
（原书p101）
